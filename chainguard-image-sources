#!/bin/bash

# This tool is implemented in shell script to provide maximum transparency in how
# it works and for the ease of extensibility by its users.
#
# The goal is for Chainguard customers to easily be able to the exact
# source code in Chainguard images
#
# Note that this script often download several GB of source code, even for the
# most basic images.
#
# This may consume a lot of network bandwidth and disk, and you may be
# throttled by upstream source repository servers.

set -e

error() {
	echo "ERROR: $@" 1>&2
	exit 1
}

info() {
	echo "INFO: $@" 1>&2
}

# Check dependencies
checkdeps() {
	for i in bunzip2 cosign jq git gzip sha512sum tar wget xz; do
		type $i 2>&1 >/dev/null || error "Please install [$i]."
	done
}

# Decode URLs
urldecode() { : "${*//+/ }"; echo -e "${_//%/\\x}"; }

usage() {
	echo "
$0 [OPTIONS] [ IMAGE[:TAG] | /path/to/file.sbom.spd ]

Options:
 -p|--platform [amd64|arm64]	Default = amd64
 --dry-run			Dry run (skip actual source downloads)

Image:		Image short name, as in the * in: cgr.dev/chainguard/*
		  - wolfi-base:latest
		  - python:3.11
		  - openjdk
		Default tag = :latest

This tool will create a new directory in your present working directory called
sources/  It will then download the SBOM associated with your specified image,
and then fetch all upstream sources with git and/or wget, at the specific
commit used in the image (git), or checked against a checksum (wget).
"
}

git_checkout() {
	local url="$1"
	# Extract the repo name (before the commit hash)
	local repo=$(echo "$url" | sed -e "s/@.*$//")
	# Extract the commit hash
	local commit=$(echo "$url" | sed -e "s/^.*@//" -e "s/#.*$//")
	# Clone the whole repo (this can be a lot -- and you might get throttled)
	# Use the fully qualified encoded URL as the directory name
	# It's pretty darn long, but it should be guaranteed unique, and it's very informative...
	info "Cloning repo [$repo]"
	[ "$DRYRUN" = "1" ] && return
	# If we already have a clean git tree, then we can skip the clone (helps with re-runs)
	if [ -d "$ref" ] && cd "$DIR/$ref" && git status 2>&1 >/dev/null; then
		info "Already have a clean working repo at [$repo]"
	else
		# Directory is busted, or doesn't exist, so clone fresh from source
		info "Need a clean working repo at [$repo]"
		rm -rf "$ref" && git clone "$repo" "$ref"
		cd "$DIR/$ref"
	fi
	# Rewind back to the exact specified commit in the sbom
	git checkout "$commit"
	cd "$DIR" 2>&1 >/dev/null
}

checkdeps
DRYRUN=0
platform="amd64"
# Handle command line options
while [ ! -z "$1" ]; do
	case "$1" in
		--dry-run)
			DRYRUN=1
			shift
		;;
		-p|--platform)
			platform="$2"
			shift 2
		;;
		-h|--help)
			usage
			exit 0
		;;
		*)
			image="$1"
			shift
		;;
	esac
done

# Make a directory to work in
mkdir -p ./sources/"$image-$platform"
cd ./sources/"$image-$platform"
DIR="$PWD"

sbom="$image-$platform.sbom.spdx"
info "Fetching sbom [$platform/$image]"
cosign download attestation \
  --platform linux/$platform \
  --predicate-type=https://spdx.dev/Document \
  cgr.dev/chainguard/$image | \
  jq '.payload | @base64d | fromjson | .predicate' > "$sbom"
info "Fetched sbom [$sbom]"

# TODO: jq would be better here, obviously, but I don't speak jq...
for ref in $(grep "[\"]referenceLocator[\"]: " "$sbom" | sed -e "s/\",$//" -e "s/^.*\"//"); do
	url=$(urldecode "$ref")
	info "Found url [$url]"
	# Handle various strategies for fetching sources
	case "$url" in
		pkg:generic/*vcs_url=git+*)
			# Strip the leading data
			url=$(echo "$url" | sed -e "s/.*git+//")
			git_checkout "$url"
		;;
		pkg:generic/*download_url=*)
			# Strip the leading data
			url=$(echo "$url" | sed -e "s/^.*download_url=//")
			# Get the checksum
			checksum_encoded=$(grep "$ref" "$sbom" | sed -e "s/.*checksum=//" -e "s/&.*$//")
			checksum=$(urldecode "$checksum_encoded")
			checksum_method=$(echo "$checksum" | sed -e "s/:.*$//")
			checksum_hash=$(echo "$checksum" | sed -e "s/^.*://")
			cmd="$checksum_method""sum"
			target_filename=$(basename "$url")
			type "$cmd" 2>&1 >/dev/null || error "Please install [$cmd]"
			info "Checking for local archive [$url]"
			[ "$DRYRUN" = "1" ] && continue
			info "Examining checksum [$checksum_hash] on [$target_filename]..."
			if [ -d "$ref" ] && cd "$ref" && [ -f "$target_filename" ] && echo "$checksum_hash" | $cmd $target_filename 2>&1 >/dev/null; then
				info "Already have a clean download of [$url]"
				cd "$DIR/$ref"
			else
				# File is busted, or doesn't exist, so download a fresh one
				rm -rf "$DIR/$ref" && mkdir -p "$DIR/$ref"
				cd "$DIR/$ref"
				info "Downloading archive [$url]"
				wget -q --continue --timestamping -O "$target_filename" "$url"
				(echo "$checksum_hash" | $cmd $target_filename 2>&1 >/dev/null) || error "Checksum does not match [$(basename $PWD)] != [$checksum_hash]"
			fi
			info "Extracting archive [$target_filename]"
			rm -rf "$DIR/$ref/*/"
			cd "$DIR/$ref"
			tar xf "$DIR/$ref/"*tar*
			cd "$DIR" 2>&1 >/dev/null
		;;
		pkg:github/*)
			# Strip the leading data
			url=$(echo "$url" | sed -e "s|^pkg:github|https://github.com|")
			git_checkout "$url"
		;;
		pkg:apk/wolfi*)
			info "WARNING: Wolfi APK Download not yet handled; skipping [$url]"
		;;
		pkg:oci/image*)
			info "WARNING: OCI Image Download not yet handled; skipping [$url]"
		;;
		*)
			# Strip the leading data
			info "WARNING: Unhandled source download method; skipping [$url]"
		;;
	esac
done
