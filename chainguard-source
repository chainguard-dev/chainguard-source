#!/bin/bash

set -e

error() {
	echo "ERROR: $@" 1>&2
	exit 1
}

info() {
	echo "INFO: $@" 1>&2
}

# Check dependencies
checkdeps() {
	for i in bunzip2 cosign jq git gzip sha512sum tar wget xz; do
		type $i 2>&1 >/dev/null || error "Please install [$i]."
	done
}

# Decode URLs
urldecode() { : "${*//+/ }"; echo -e "${_//%/\\x}"; }

usage() {
	echo "
$0 [OPTIONS] [ IMAGE[:TAG] | SBOM.spdx.json ]

Options:
 -p|--platform [amd64|arm64]	Default = amd64
 --dry-run			Dry run (skip actual source downloads)
 -y|--yes			Automatically answer "y" to acknowledge the
 				warning prompt that this tool will use a lot of
				network and disk

Image:		Image short name, as in the * in: cgr.dev/chainguard/*
		  - wolfi-base:latest
		  - python:3.11
		  - openjdk
		Default tag = :latest

SBOM.spdx.json:	Path to a Chainguard JSON SPDX file name

Notes:
  This tool will create a new directory in your present working directory called
  sources/  It will then download the SBOM associated with your specified image,
  and then fetch all upstream sources with git and/or wget, at the specific
  commit used in the image (git), or checked against a checksum (wget).

  This tool is implemented in shell script to provide maximum transparency in how
  it works and for the ease of extensibility by its users.

  The goal is for Chainguard customers to easily be able to the exact
  source code in Chainguard images

  Note that this script often download several GB of source code, even for the
  most basic images.

  This may consume a lot of network bandwidth and disk, and you may be
  throttled by upstream source repository servers.
"
}

git_checkout() {
	local url="$1"
	# Extract the repo name (before the commit hash)
	local repo=$(echo "$url" | sed -e "s/@.*$//")
	# Extract the commit hash
	local commit=$(echo "$url" | sed -e "s/^.*@//" -e "s/#.*$//")
	# Clone the whole repo (this can be a lot -- and you might get throttled)
	# Use the fully qualified encoded URL as the directory name
	# It's pretty darn long, but it should be guaranteed unique, and it's very informative...
	info "Cloning repo [$repo]"
	[ "$DRYRUN" = "1" ] && return
	# If we already have a clean git tree, then we can skip the clone (helps with re-runs)
	if [ -d "$ref" ] && cd "$DIR/$ref" && git status 2>&1 >/dev/null; then
		info "Already have a clean working repo at [$repo]"
	else
		# Directory is busted, or doesn't exist, so clone fresh from source
		info "Need a clean working repo at [$repo]"
		rm -rf "$ref" && git clone "$repo" "$ref"
		cd "$DIR/$ref"
	fi
	# Rewind back to the exact specified commit in the sbom
	git checkout "$commit"
	cd "$DIR" 2>&1 >/dev/null
}

checkdeps
DRYRUN=0
YES=0
PLATFORM="amd64"
# Handle command line options
while [ ! -z "$1" ]; do
	case "$1" in
		--dry-run)
			DRYRUN=1
			shift
		;;
		-y|--yes)
			YES=1
			shift
		;;
		-p|--platform)
			PLATFORM="$2"
			shift 2
		;;
		-h|--help)
			YES=1
			usage
			exit 0
		;;
		*)
			TARGET="$1"
			shift
		;;
	esac
done

if [ "$YES" != "1" ]; then
	# Warn that this tool may consume a lot of network and disk
	echo
	echo "This tool will consume a lot of network and disk, as it fetches full"
	echo "git source code repositories and source archive files from remote"
	echo "servers (outside of chainguard.dev)."
	echo
	echo "You will likely need several GB of free disk space in your current"
	echo "working directory [$(pwd)]:"
	df -h .
	echo
	echo "And this will consume a lot of network bandwith."
	echo "Your connection may be throttled by the remote servers."
	echo
	while true; do
		echo -n "Do you want to continue? [y/n]: "
		read -n1 answer
		case "$answer" in
			y|Y)
				echo
				break
			;;
			*)
				echo
				exit 1
			;;
		esac
	done
fi

# Handle target sbom
if [ -f "$TARGET" ]; then
	# Target is a local file
	# Make a directory to work in
	mkdir -p ./sources/"$(basename $TARGET)"
	cd ./sources/"$(basename $TARGET)"
	DIR=$(pwd)
	sbom="$DIR/$(basename $TARGET)"
	cat "$TARGET" > "$sbom"
else
	# Try to determine and retrieve sbom over network
	# Make a directory to work in
	image="$TARGET"
	mkdir -p ./sources/"$image-$PLATFORM"
	cd ./sources/"$image-$PLATFORM"
	DIR=$(pwd)
	sbom="$DIR/$image-$PLATFORM.sbom.spdx.json"
	info "Fetching sbom [$PLATFORM/$image]"
	cosign download attestation \
	  --platform linux/$PLATFORM \
	  --predicate-type=https://spdx.dev/Document \
	  cgr.dev/chainguard/$image | \
	  jq '.payload | @base64d | fromjson | .predicate' > "$sbom"
	info "Fetched sbom [$sbom]"
fi


for ref in $(jq < "$sbom" -r '.packages[].externalRefs[].referenceLocator'); do
	url=$(urldecode "$ref")
	info "Found url [$url]"
	# Handle various strategies for fetching sources
	case "$url" in
		pkg:generic/*vcs_url=git+*)
			# Strip the leading data
			url=$(echo "$url" | sed -e "s/.*git+//")
			git_checkout "$url"
		;;
		pkg:generic/*download_url=*)
			# Strip the leading data
			url=$(echo "$url" | sed -e "s/^.*download_url=//")
			# Get the checksum
			checksum_encoded=$(grep "$ref" "$sbom" | sed -e "s/.*checksum=//" -e "s/&.*$//")
			checksum=$(urldecode "$checksum_encoded")
			checksum_method=$(echo "$checksum" | sed -e "s/:.*$//")
			checksum_hash=$(echo "$checksum" | sed -e "s/^.*://")
			cmd="$checksum_method""sum"
			target_filename=$(basename "$url")
			type "$cmd" 2>&1 >/dev/null || error "Please install [$cmd]"
			info "Checking for local archive [$url]"
			[ "$DRYRUN" = "1" ] && continue
			info "Examining checksum [$checksum_hash] on [$target_filename]..."
			if [ -d "$ref" ] && cd "$ref" && [ -f "$target_filename" ] && echo "$checksum_hash" | $cmd $target_filename 2>&1 >/dev/null; then
				info "Already have a clean download of [$url]"
				cd "$DIR/$ref"
			else
				# File is busted, or doesn't exist, so download a fresh one
				rm -rf "$DIR/$ref" && mkdir -p "$DIR/$ref"
				cd "$DIR/$ref"
				info "Downloading archive [$url]"
				wget -q --continue --timestamping -O "$target_filename" "$url"
				(echo "$checksum_hash" | $cmd $target_filename 2>&1 >/dev/null) || error "Checksum does not match [$(basename $(pwd))] != [$checksum_hash]"
			fi
			info "Extracting archive [$target_filename]"
			rm -rf "$DIR/$ref/*/"
			cd "$DIR/$ref"
			tar xf "$DIR/$ref/"*tar*
			cd "$DIR" 2>&1 >/dev/null
		;;
		pkg:github/*)
			# Strip the leading data
			url=$(echo "$url" | sed -e "s|^pkg:github|https://github.com|")
			git_checkout "$url"
		;;
		pkg:apk/wolfi*)
			info "WARNING: Wolfi APK Download not yet handled; skipping [$url]"
		;;
		pkg:oci/image*)
			info "WARNING: OCI Image Download not yet handled; skipping [$url]"
		;;
		*)
			# Strip the leading data
			info "WARNING: Unhandled source download method; skipping [$url]"
		;;
	esac
done
